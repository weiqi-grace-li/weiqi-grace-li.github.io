research_projects:

  - title: "Benchmarking Cell-Type Specific Spatially Variable Gene Detection Methods Using a Realistic and Decomposable Simulation Framework"
    author_text: "**Weiqi Li**,  Xinzhou Ge†, Yuan Jiang†"
    image: "research/r1.png"
    status_info: "Preprint bioAxiv, 2025"
    links:
      - url: "research/r1.pdf"
        name: "DOI"
        # 本地连接
        type: "local"
      - url: "https://github.com/weiqi-grace-li/ctSVG-benchmarking"
        name: "CODE"
    abstract: |
      Identifying spatially variable genes within individual cell types is essential for characterizing spatially organized cell states and microenvironments from spatial transcriptomics data.  Several computational methods have been developed for identifying cell-type-specific spatially variable genes (ctSVGs), but their relative performance and practical utility under realistic biological complexity remain largely unknown.  To address this gap, we present the first systematic benchmark study of all five existing ctSVG detection methods--CELINA, STANCE, C-SIDE, CTSV and spVC--using an integrated evaluation framework that combines idealized simulations, Xenium-based realistic simulations, and a decomposition-based diagnostic analysis. We compared the methods in terms of detection accuracy, scalability, and usability. Across realistic datasets generated on various tissue types, all methods experienced sharp declines in detection accuracy and substantial inflation of false discoveries compared to idealized simulations. To explain this failure, we developed a new simulation framework that decomposes the “realness” of the realistic simulation into interpretable biological and technical components, enabling us to attribute method-specific performance losses to specific components, including realistic diversity of cell types, heterogeneous cell layouts, null gene distributions, capture efficiency and realistic intra-cell-type spatial patterns. Together, our results show that no single method dominates across detection accuracy, scalability and usability, and we further clarify why current ctSVG methods fall short in realistic settings. We summarize these tradeoffs into a practical user guide to support method selection and highlight key challenges in developing robust, scalable ctSVG detection tools for real spatial transcriptomics data.



  - title: "Reframing Spatial Transcriptomics Prediction: From Regression to Classification Using Swin Transformer"
    author_text: "**Weiqi Li**,  Fuxin Li, Xinzhou Ge, Yuan Jiang†"
    image: "research/r2.png"
    status_info: "Submitted to ENAR Distinguished Student Paper Award Competition, 2026"
    links:
      - url: "research/r2.pdf"
        name: "Manuscript"
        # 本地连接
        type: "local"
      - url: "https://github.com/weiqi-grace-li/swin-classy"
        name: "CODE"
    abstract: |
      Spatial transcriptomics profiles gene expression while preserving spatial context, offering unprecedented insight into tissue organization. However, high cost and technical complexity limit its routine clinical adoption. A promising alternative is to predict spatially resolved expression directly from widely available histology images using deep learning. Existing approaches formulate this as a regression task, attempting to predict continuous gene expression values from small cropped H&E image patches. We argue this formulation is ill-suited for sparse, discrete, and noisy expression data. In this study, we reframe the task as a multi-class classification problem, predicting discrete expression levels rather than exact values. We demonstrate that this reformulation substantially improves discriminative performance and can be seamlessly incorporated into existing architectures. We further propose a Swin Transformer-based framework that efficiently captures hierarchical spatial context while reducing model complexity and enhancing scalability.  The proposed model outperforms the CNN-based benchmark even with limited neighborhood context, offering a viable and scalable solution for near-cellular and subcellular resolution spatial transcriptomics datasets



  - title: "CASTLE: Cell Annotation in Spatial Transcriptomics through Learned Embedding Clustering"
    author_text: "**Weiqi Li**,  Yun-Kuei Lin, Alina Hyk"
    image: "research/r3.png"
    status_info: "Manuscript in preparation"
    links:
      - url: "research/r3.pdf"
        name: "Manuscript"
        # 本地连接
        type: "local"
      - url: "https://github.com/weiqi-grace-li/CASTLE"
        name: "CODE"
    abstract: |
      Cell type annotation in Spatial Transcriptomics datasets is a critical step for downstream genomic analysis.  However, existing methods fail to take full advantage of the rich information.  They also rely heavily on external datasets and frame the problem as a classification task, which overlook intrinsic dataset structure and limit discovery of novel cell types.  We propose CASTLE, a clustering framework that integrates foundational models for multi-modal feature extraction and uses Deep Embedding Clustering to jointly optimize representation learning and clustering.  Our results suggest encoded multi-modal features can increase the separability of among cell groups, providing key insights into the role of foundational model embeddings in enhancing cluster separation.  We also demonstrate the superiority of Stacked Denoising AutoEncoder (SDAE) for fusing multi-modal features comparing to conventional clustering such as PCA and UMAP.

